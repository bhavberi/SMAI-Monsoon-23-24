{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../Datasets/HousingData.csv\"\n",
    "\n",
    "random_state = 42\n",
    "wandb_log = False\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\n",
    "hyperparameter_tuning = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "max_epochs = 1000\n",
    "optimizer = 'bgd'\n",
    "activation = 'sigmoid'\n",
    "hidden_layers = [8,]\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandb_init(lr, max_epochs, optimizer, activation, hidden_layers, batch_size):\n",
    "    if wandb_log:\n",
    "        config = {\n",
    "            \"lr\": lr, \n",
    "            \"model_type\": \"MLP_Regression\",\n",
    "            \"optimizer\": optimizer, # SGC/BGD/MBGD\n",
    "            \"criterion\": \"mse\",\n",
    "            \"num_epochs\": max_epochs,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"hidden_layers\": hidden_layers,\n",
    "            \"activation\": activation,\n",
    "            \"wandb_run_name\": \"bhav\" ,\n",
    "            \"tags\": [\"P3\",]\n",
    "        }\n",
    "\n",
    "        wandb.init(entity = \"bhavberi\",   # wandb username. (NOT REQUIRED ARG. ANYMORE, it fetches from initial login)\n",
    "                project = \"SMAI\",        # wandb project name. New project will be created if given project is missing.\n",
    "                config = config         # Config dict\n",
    "                )\n",
    "        wandb.run.name = f\"P3_{config['optimizer']}_{config['activation']}_{len(config['hidden_layers'])}_{config['lr']}_{config['batch_size']}_{config['num_epochs']}\"\n",
    "        print(wandb.run.name)\n",
    "\n",
    "def wandb_finish():\n",
    "    if wandb_log:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  PTRATIO  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "        B  LSTAT  MEDV  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90    NaN  36.2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(dataset_path)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>486.000000</td>\n",
       "      <td>486.000000</td>\n",
       "      <td>486.000000</td>\n",
       "      <td>486.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>486.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>486.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.611874</td>\n",
       "      <td>11.211934</td>\n",
       "      <td>11.083992</td>\n",
       "      <td>0.069959</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.518519</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.715432</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.720192</td>\n",
       "      <td>23.388876</td>\n",
       "      <td>6.835896</td>\n",
       "      <td>0.255340</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>27.999513</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.155871</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.081900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.175000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>7.125000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.253715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>76.800000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.430000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.560263</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>93.975000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  486.000000  486.000000  486.000000  486.000000  506.000000  506.000000   \n",
       "mean     3.611874   11.211934   11.083992    0.069959    0.554695    6.284634   \n",
       "std      8.720192   23.388876    6.835896    0.255340    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.081900    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.253715    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.560263   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  486.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.518519    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     27.999513    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.175000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     76.800000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     93.975000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT        MEDV  \n",
       "count  486.000000  506.000000  \n",
       "mean    12.715432   22.532806  \n",
       "std      7.155871    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      7.125000   17.025000  \n",
       "50%     11.430000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop(columns=['MEDV']).to_numpy()\n",
    "y = dataset['MEDV'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.hist(y, bins=30, color='skyblue', edgecolor='black')\n",
    "# plt.title('Distribution of Housing Prices in Boston')\n",
    "# plt.xlabel('Housing Price')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.grid(True)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "# To fill in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (404, 13) (404,)\n",
      "Validation set shape: (51, 13) (51,)\n",
      "Test set shape: (51, 13) (51,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=random_state)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set shape:\", X_validation.shape, y_validation.shape)\n",
    "print(\"Test set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_validation_std = scaler.transform(X_validation)\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Regression:\n",
    "    def __init__(self, input_size, hidden_layers, output_size, learning_rate=0.01, activation='sigmoid', optimizer='sgd', print_every=10, wandb_log=False):\n",
    "        assert activation.lower() in ['sigmoid', 'relu', 'tanh'], \"Activation function must be either 'sigmoid', 'relu' or 'tanh'\"\n",
    "        assert optimizer.lower() in ['sgd', 'bgd', 'mbgd'], \"Optimizer must be either 'sgd', 'bgd' or 'mbgd'\"\n",
    "        assert input_size > 0, \"Input size must be greater than 0\"\n",
    "        assert output_size > 0, \"Output size must be greater than 0\"\n",
    "        assert learning_rate > 0, \"Learning rate must be greater than 0\"\n",
    "        assert type(hidden_layers) == list and len(hidden_layers) > 0, \"Hidden layers must be a list of size greater than 0\"\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.activation_func = self._get_activation_func(activation)\n",
    "        self.optimizer_func = self._get_optimizer_func(optimizer)\n",
    "        self.weights, self.biases = self._initialize_weights_and_biases()\n",
    "\n",
    "        self.wandb_log = wandb_log\n",
    "        self.print_every = print_every\n",
    "\n",
    "    # Activation functions\n",
    "    def _get_activation_func(self, activation):\n",
    "        if activation == \"sigmoid\":\n",
    "            return self._sigmoid\n",
    "        elif activation == \"tanh\":\n",
    "            return self._tanh\n",
    "        elif activation == \"relu\":\n",
    "            return self._relu\n",
    "        else:\n",
    "            raise ValueError(f\"Activation function '{activation}' not supported.\")\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def _tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def _relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    # Activation derivative\n",
    "    def _activation_derivative(self, Z):\n",
    "        if self.activation_func == self._sigmoid:\n",
    "            return self._sigmoid_derivative(Z)\n",
    "        elif self.activation_func == self._tanh:\n",
    "            return self._tanh_derivative(Z)\n",
    "        elif self.activation_func == self._relu:\n",
    "            return self._relu_derivative(Z)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Activation function '{self.activation_func}' not supported.\"\n",
    "            )\n",
    "\n",
    "    def _sigmoid_derivative(self, Z):\n",
    "        return self._sigmoid(Z) * (1 - self._sigmoid(Z))\n",
    "\n",
    "    def _tanh_derivative(self, Z):\n",
    "        return 1 - np.square(self._tanh(Z))\n",
    "\n",
    "    def _relu_derivative(self, Z):\n",
    "        return np.where(Z > 0, 1, 0)\n",
    "\n",
    "    # Optimizers\n",
    "    def _get_optimizer_func(self, optimizer):\n",
    "        if optimizer == \"sgd\":\n",
    "            return self._sgd\n",
    "        elif optimizer == \"bgd\":\n",
    "            return self._bgd\n",
    "        elif optimizer == \"mbgd\":\n",
    "            return self._mbgd\n",
    "        else:\n",
    "            raise ValueError(f\"Optimizer '{optimizer}' not supported.\")\n",
    "\n",
    "    def _sgd(self, grads):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * grads[\"dW\"][i]\n",
    "            self.biases[i] -= self.learning_rate * grads[\"db\"][i]\n",
    "\n",
    "    def _bgd(self, grads):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * grads[\"dW\"][i] / self.input_size\n",
    "            self.biases[i] -= self.learning_rate * grads[\"db\"][i] / self.input_size\n",
    "\n",
    "    def _mbgd(self, grads):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= (\n",
    "                self.learning_rate * grads[\"dW\"][i] / grads[\"dW\"][i].shape[1]\n",
    "            )\n",
    "            self.biases[i] -= (\n",
    "                self.learning_rate * grads[\"db\"][i] / grads[\"db\"][i].shape[1]\n",
    "            )\n",
    "\n",
    "    # Initialize weights and biases\n",
    "    def _initialize_weights_and_biases(self):\n",
    "        num_layers = len(self.hidden_layers)\n",
    "        weights = []\n",
    "        biases = []\n",
    "\n",
    "        if num_layers == 0:\n",
    "            w = np.random.randn(self.input_size, self.output_size)\n",
    "            b = np.zeros((1, self.output_size))\n",
    "            weights.append(w)\n",
    "            biases.append(b)\n",
    "            return weights, biases\n",
    "\n",
    "        # Using Github Copilot\n",
    "        for i in range(num_layers + 1):\n",
    "            if i == 0:\n",
    "                w = 0.01 * np.random.randn(self.input_size, self.hidden_layers[0])\n",
    "            elif i == num_layers:\n",
    "                w = 0.01 * np.random.randn(self.hidden_layers[-1], self.output_size)\n",
    "            else:\n",
    "                w = 0.01 * np.random.randn(\n",
    "                    self.hidden_layers[i - 1], self.hidden_layers[i]\n",
    "                )\n",
    "\n",
    "            b = np.zeros((1, w.shape[1]))\n",
    "            weights.append(w)\n",
    "            biases.append(b)\n",
    "\n",
    "        return weights, biases\n",
    "\n",
    "    # Forward propagation\n",
    "    def _forward_propagation(self, X):\n",
    "        num_layers = len(self.weights)\n",
    "        A = X\n",
    "        caches = []\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            W = self.weights[i]\n",
    "            b = self.biases[i]\n",
    "\n",
    "            Z = np.dot(A, W) + b\n",
    "\n",
    "            if Z.shape[1] == 1:\n",
    "                Z = Z.reshape(-1)\n",
    "            caches.append((A, W, b, Z))\n",
    "            # print(\"Forward\", A.shape, W.shape, b.shape, Z.shape)\n",
    "\n",
    "            A = self.activation_func(Z)\n",
    "\n",
    "        if len(A.shape) == 1:\n",
    "            A = A.reshape(-1)\n",
    "        return A, caches\n",
    "\n",
    "    # Backward propagation\n",
    "    def _backward_propagation(self, A, Y, caches):\n",
    "        num_samples = A.shape[0]\n",
    "        num_layers = len(self.weights)\n",
    "        grads = {\"dW\": [], \"db\": []}\n",
    "\n",
    "        delta = A - Y\n",
    "        # print(delta.shape, A.shape, Y.shape)\n",
    "\n",
    "        for i in reversed(range(num_layers)):\n",
    "            A, W, _, Z = caches[i]\n",
    "            # print(\"A\", A.shape, \"W\", W.shape, \"Z\", Z.shape)\n",
    "\n",
    "            dZ = np.multiply(delta, self._activation_derivative(Z))\n",
    "            if dZ.ndim == 1:\n",
    "                dZ = dZ.reshape((dZ.shape[0], 1))\n",
    "            # print(\"dZ\", dZ.shape)\n",
    "            dW = np.dot(A.T, dZ)\n",
    "            # print(\"dW\", dW.shape)\n",
    "            db = np.sum(dZ, axis=0, keepdims=True)\n",
    "            # print(\"db\", db.shape)\n",
    "\n",
    "            delta = np.dot(dZ, W.T)\n",
    "            # print(\"delta\", delta.shape)\n",
    "\n",
    "            if len(dW.shape) == 1:\n",
    "                dW = dW.reshape(-1, 1)\n",
    "\n",
    "            grads[\"dW\"].append(dW)\n",
    "            grads[\"db\"].append(db)\n",
    "\n",
    "        grads[\"dW\"].reverse()\n",
    "        grads[\"db\"].reverse()\n",
    "\n",
    "        return grads\n",
    "\n",
    "    # Calculate cost\n",
    "    def _calculate_cost(self, A, Y):\n",
    "        # print(A.shape, Y.shape)\n",
    "        mse = np.mean(np.square(A - Y))\n",
    "        rmse = np.sqrt(mse)\n",
    "        r_squred = 1 - (np.sum(np.square(Y - A)) / np.sum(np.square(Y - np.mean(Y))))\n",
    "        return mse, rmse, r_squred\n",
    "\n",
    "    # Predict\n",
    "    def predict(self, X):\n",
    "        A, _ = self._forward_propagation(X)\n",
    "        return A\n",
    "    \n",
    "    # Evaluate\n",
    "    def evaluate(self, X, Y):\n",
    "        A = self.predict(X)\n",
    "        mse, rmse, r_squared = self._calculate_cost(A, Y)\n",
    "        return mse, rmse, r_squared\n",
    "\n",
    "    # Train the model\n",
    "    def train(\n",
    "        self, X, Y, max_epochs=10, batch_size=32, X_validation=None, y_validation=None\n",
    "    ):\n",
    "        num_samples = X.shape[0]\n",
    "        costs = []\n",
    "\n",
    "        for i in range(max_epochs):\n",
    "            if self.optimizer == \"bgd\":\n",
    "                batch_size = num_samples\n",
    "                num_batches = 1\n",
    "            elif self.optimizer == \"sgd\":\n",
    "                batch_size = 1\n",
    "                num_batches = num_samples\n",
    "            elif self.optimizer == \"mbgd\":\n",
    "                num_batches = num_samples // batch_size\n",
    "            else:\n",
    "                raise ValueError(f\"Optimizer '{self.optimizer}' not supported.\")\n",
    "\n",
    "            for j in range(num_batches):\n",
    "                start = j * batch_size\n",
    "                end = start + batch_size\n",
    "\n",
    "                A, caches = self._forward_propagation(X[start:end])\n",
    "                grads = self._backward_propagation(A, Y[start:end], caches)\n",
    "                self.optimizer_func(grads)\n",
    "\n",
    "            A = self.predict(X)\n",
    "            mse, rmse, r_squared = self._calculate_cost(A, Y)\n",
    "            costs.append(mse)\n",
    "\n",
    "            data_to_log = {\"epoch\": i + 1, \"train_loss\": mse}\n",
    "\n",
    "            # Calculate validation loss\n",
    "            if X_validation is not None and y_validation is not None:\n",
    "                A = self.predict(X_validation)\n",
    "                val_loss_mse, val_loss_rmse, val_loss_r_squared = self._calculate_cost(A, y_validation)\n",
    "                data_to_log[\"val_loss\"] = val_loss_mse\n",
    "\n",
    "            if self.wandb_log:\n",
    "                wandb.log(data_to_log)\n",
    "\n",
    "            if self.print_every and (i + 1) % self.print_every == 0:\n",
    "                print(f\"Cost after {i+1} epochs: {mse}\")\n",
    "\n",
    "        return costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after 10 epochs: 19.698749879548878\n",
      "Cost after 20 epochs: 16.557903666103567\n",
      "Cost after 30 epochs: 14.56211850701852\n",
      "Cost after 40 epochs: 13.332479688206348\n",
      "Cost after 50 epochs: 12.313574849265848\n",
      "Cost after 60 epochs: 11.661011897596628\n",
      "Cost after 70 epochs: 11.017753273562787\n",
      "Cost after 80 epochs: 10.594313452696289\n",
      "Cost after 90 epochs: 10.169950167299316\n",
      "Cost after 100 epochs: 9.989003266184923\n"
     ]
    }
   ],
   "source": [
    "model = MLP_Regression(X_train_std.shape[1], hidden_layers, 1, learning_rate=0.001, activation=\"relu\", optimizer=\"mbgd\")\n",
    "costs = model.train(X_train_std, y_train, max_epochs=100, batch_size=batch_size, X_validation=X_validation_std, y_validation=y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(costs)\n",
    "# plt.title('Training Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1763511523476295\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(y_test, model.predict(X_test_std)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyperparameter_tuning:\n",
    "    for optimizer in ['sgd', 'bgd', 'mbgd']:\n",
    "        for activation in ['sigmoid', 'relu', 'tanh']:\n",
    "            for lr in [0.001, 0.01, 0.1]:\n",
    "                max_epochs = 1000\n",
    "                batch_size = 32\n",
    "                wandb_init(lr, max_epochs, optimizer, activation, hidden_layers, batch_size)\n",
    "                model = MLP_Regression(X_train_std.shape[1], hidden_layers, 1, learning_rate=lr, activation=activation, optimizer=optimizer, print_every=None, wandb_log=True)\n",
    "                costs = model.train(X_train_std, y_train, max_epochs=max_epochs, batch_size=batch_size, X_validation=X_validation_std, y_validation=y_validation)\n",
    "                mse, rmse, r_squared = model.evaluate(X_test_std, y_test)\n",
    "                wandb.log({\"test_loss\": mse, \"test_rmse\": rmse, \"test_r_squared\": r_squared})\n",
    "                wandb_finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Name activation     lr optimizer  batch_size  \\\n",
      "0   P3_bgd_relu_1_0.001_32_1000       relu  0.001       bgd          32   \n",
      "1   P3_sgd_relu_1_0.001_32_1000       relu  0.001       sgd          32   \n",
      "2  P3_mbgd_relu_1_0.001_32_1000       relu  0.001      mbgd          32   \n",
      "3   P3_mbgd_relu_1_0.01_32_1000       relu  0.010      mbgd          32   \n",
      "4    P3_sgd_relu_1_0.01_32_1000       relu  0.010       sgd          32   \n",
      "\n",
      "   num_epochs hidden_layers  train_loss   val_loss  test_loss  test_rmse  \\\n",
      "0        1000           [8]    9.426676  19.699096  10.451521   3.232881   \n",
      "1        1000           [8]    7.602558  20.720354   7.488175   2.736453   \n",
      "2        1000           [8]    7.401104  23.469117   9.675291   3.110513   \n",
      "3        1000           [8]   20.694094  26.590245  19.647841   4.432589   \n",
      "4        1000           [8]   36.851673  30.856010  31.890613   5.647177   \n",
      "\n",
      "   test_r_squared  \n",
      "0        0.874008  \n",
      "1        0.909731  \n",
      "2        0.883365  \n",
      "3        0.763147  \n",
      "4        0.615562  \n"
     ]
    }
   ],
   "source": [
    "wandb_data = pd.read_csv(\"../wandb_export_P3.csv\")\n",
    "print(wandb_data.head())\n",
    "\n",
    "wandb_data.sort_values(by=[\"test_loss\", \"test_rmse\", \"test_r_squared\"], ascending=True, inplace=True)\n",
    "wandb_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Name   test_loss  test_rmse  test_r_squared\n",
      "0       P3_sgd_relu_1_0.001_32_1000    7.488175   2.736453        0.909731\n",
      "1      P3_mbgd_relu_1_0.001_32_1000    9.675291   3.110513        0.883365\n",
      "2       P3_bgd_relu_1_0.001_32_1000   10.451521   3.232881        0.874008\n",
      "3       P3_mbgd_relu_1_0.01_32_1000   19.647841   4.432589        0.763147\n",
      "4        P3_sgd_relu_1_0.01_32_1000   31.890613   5.647177        0.615562\n",
      "5         P3_sgd_relu_1_0.1_32_1000  220.656171  14.854500       -1.659987\n",
      "6        P3_mbgd_tanh_1_0.1_32_1000  484.131176  22.002981       -4.836151\n",
      "7         P3_bgd_tanh_1_0.1_32_1000  484.131176  22.002981       -4.836151\n",
      "8     P3_mbgd_sigmoid_1_0.1_32_1000  484.131176  22.002981       -4.836151\n",
      "9      P3_bgd_sigmoid_1_0.1_32_1000  484.131176  22.002981       -4.836151\n",
      "10        P3_sgd_tanh_1_0.1_32_1000  484.131180  22.002981       -4.836151\n",
      "11     P3_sgd_sigmoid_1_0.1_32_1000  484.131185  22.002981       -4.836152\n",
      "12       P3_sgd_tanh_1_0.01_32_1000  484.131211  22.002982       -4.836152\n",
      "13      P3_mbgd_tanh_1_0.01_32_1000  484.131220  22.002982       -4.836152\n",
      "14       P3_bgd_tanh_1_0.01_32_1000  484.131231  22.002982       -4.836152\n",
      "15    P3_sgd_sigmoid_1_0.01_32_1000  484.131268  22.002983       -4.836153\n",
      "16   P3_mbgd_sigmoid_1_0.01_32_1000  484.131329  22.002985       -4.836153\n",
      "17      P3_sgd_tanh_1_0.001_32_1000  484.131597  22.002991       -4.836156\n",
      "18   P3_sgd_sigmoid_1_0.001_32_1000  484.132216  22.003005       -4.836164\n",
      "19     P3_mbgd_tanh_1_0.001_32_1000  484.132365  22.003008       -4.836166\n",
      "20    P3_bgd_sigmoid_1_0.01_32_1000  484.132583  22.003013       -4.836168\n",
      "21  P3_mbgd_sigmoid_1_0.001_32_1000  484.132718  22.003016       -4.836170\n",
      "22      P3_bgd_tanh_1_0.001_32_1000  484.141531  22.003216       -4.836276\n",
      "23   P3_bgd_sigmoid_1_0.001_32_1000  484.146978  22.003340       -4.836342\n",
      "24       P3_bgd_relu_1_0.01_32_1000  525.190000  22.917024       -5.331111\n",
      "25       P3_mbgd_relu_1_0.1_32_1000  525.190000  22.917024       -5.331111\n",
      "26        P3_bgd_relu_1_0.1_32_1000  525.190000  22.917024       -5.331111\n"
     ]
    }
   ],
   "source": [
    "columns = [\"Name\", \"test_loss\", \"test_rmse\", \"test_r_squared\"]\n",
    "print(wandb_data[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for MLP Regression:\n",
      "Optimizer: sgd\n",
      "Activation: relu\n",
      "Learning Rate: 0.001\n",
      "Batch Size: 32\n",
      "Epochs: 1000\n",
      "Hidden Layers: [8]\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Parameters for MLP Regression:\")\n",
    "print(\"Optimizer:\", wandb_data.iloc[0][\"optimizer\"])\n",
    "print(\"Activation:\", wandb_data.iloc[0][\"activation\"])\n",
    "print(\"Learning Rate:\", wandb_data.iloc[0][\"lr\"])\n",
    "print(\"Batch Size:\", wandb_data.iloc[0][\"batch_size\"])\n",
    "print(\"Epochs:\", wandb_data.iloc[0][\"num_epochs\"])\n",
    "print(\"Hidden Layers:\", wandb_data.iloc[0][\"hidden_layers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 6.413082090060536\n",
      "Test RMSE: 2.532406383276692\n",
      "Test R^2: 0.9226909564902691\n"
     ]
    }
   ],
   "source": [
    "best_model = MLP_Regression(X_train_std.shape[1], hidden_layers, 1, learning_rate=wandb_data.iloc[0][\"lr\"], activation=wandb_data.iloc[0][\"activation\"], optimizer=wandb_data.iloc[0][\"optimizer\"], print_every=None, wandb_log=False)\n",
    "costs = best_model.train(X_train_std, y_train, max_epochs=wandb_data.iloc[0][\"num_epochs\"], batch_size=wandb_data.iloc[0][\"batch_size\"], X_validation=X_validation_std, y_validation=y_validation)\n",
    "\n",
    "mse, rmse, r_squared = best_model.evaluate(X_test_std, y_test)\n",
    "print(\"Test Loss:\", mse)\n",
    "print(\"Test RMSE:\", rmse)\n",
    "print(\"Test R^2:\", r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
